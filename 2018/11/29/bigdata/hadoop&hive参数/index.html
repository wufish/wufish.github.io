<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>hadoop&amp;hive 参数 | wufish的博客</title><meta name="keywords" content="learning,hadoop,hive"><meta name="author" content="wufish,wuzhijunw@163.com"><meta name="copyright" content="wufish"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="[TOC]"><meta property="og:type" content="article"><meta property="og:title" content="hadoop&amp;hive 参数"><meta property="og:url" content="https://wufish.github.io/2018/11/29/bigdata/hadoop&hive%E5%8F%82%E6%95%B0/index.html"><meta property="og:site_name" content="wufish的博客"><meta property="og:description" content="[TOC]"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><meta property="article:published_time" content="2018-11-28T16:00:00.000Z"><meta property="article:modified_time" content="2020-10-04T07:20:16.480Z"><meta property="article:author" content="wufish"><meta property="article:tag" content="learning"><meta property="article:tag" content="hadoop"><meta property="article:tag" content="hive"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="https://wufish.github.io/2018/11/29/bigdata/hadoop&amp;hive%E5%8F%82%E6%95%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="yandex-verification" content="{&quot;enable&quot;:true,&quot;main&quot;:&quot;#49B1F5&quot;,&quot;paginator&quot;:&quot;#00c4b6&quot;,&quot;button_hover&quot;:&quot;#FF7242&quot;,&quot;text_selection&quot;:&quot;#00c4b6&quot;,&quot;link_color&quot;:&quot;#99a9bf&quot;,&quot;meta_color&quot;:&quot;#858585&quot;,&quot;hr_color&quot;:&quot;#A4D8FA&quot;,&quot;code_foreground&quot;:&quot;#F47466&quot;,&quot;code_background&quot;:&quot;rgba(27, 31, 35, .05)&quot;,&quot;toc_color&quot;:&quot;#00c4b6&quot;,&quot;blockquote_padding_color&quot;:&quot;#49b1f5&quot;,&quot;blockquote_background_color&quot;:&quot;#49b1f5&quot;}"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: wufish","link":"链接: ","source":"来源: wufish的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isSidebar:!0,postUpdate:"2020-10-04 15:20:16"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/images/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">26</div></a></div></div><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">4</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于我</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop"><span class="toc-number">1.</span> <span class="toc-text">hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#notice"><span class="toc-number">1.1.</span> <span class="toc-text">notice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4"><span class="toc-number">1.2.</span> <span class="toc-text">hdfs 操作指令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">hadoop优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mr-job-%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">mr job 流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#map"><span class="toc-number">1.3.2.</span> <span class="toc-text">map</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#shuffle"><span class="toc-number">1.3.3.</span> <span class="toc-text">shuffle</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#reduce"><span class="toc-number">1.3.4.</span> <span class="toc-text">reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%9C%BA%E6%99%AF"><span class="toc-number">1.3.5.</span> <span class="toc-text">优化场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hadoop%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-number">1.4.</span> <span class="toc-text">hadoop配置说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive"><span class="toc-number">2.</span> <span class="toc-text">hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#notice-1"><span class="toc-number">2.1.</span> <span class="toc-text">notice</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hql"><span class="toc-number">2.2.</span> <span class="toc-text">hql</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive-%E5%92%8Chadoop-%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-number">2.3.</span> <span class="toc-text">hive 和hadoop 配置说明</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#hive-%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-number">2.3.1.</span> <span class="toc-text">hive 配置说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hadoop-%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-number">2.3.2.</span> <span class="toc-text">hadoop 配置说明</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive-reduce%E6%9C%80%E7%BB%88%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%8E%8B%E7%BC%A9"><span class="toc-number">3.</span> <span class="toc-text">hive reduce最终数据不压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hive-Job-%E7%BB%86%E8%8A%82%E4%BC%98%E5%8C%96"><span class="toc-number">3.1.</span> <span class="toc-text">hive Job 细节优化</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image:url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">wufish的博客</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>关于我</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">hadoop&amp;hive 参数</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2018-11-28T16:00:00.000Z" title="发表于 2018-11-29 00:00:00">2018-11-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-10-04T07:20:16.480Z" title="更新于 2020-10-04 15:20:16">2020-10-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2018/11/29/bigdata/hadoop&amp;hive%E5%8F%82%E6%95%B0/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>[TOC]</p><a id="more"></a><h2 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h2><h3 id="notice"><a href="#notice" class="headerlink" title="notice"></a>notice</h3><ul><li><p>Map和Reduce阶段不能对集合进行写操作。即使写了在reduce阶段也读不出来。</p><p>map和reduce会在不同的服务器上操作，全局变量无法生效。<br>1、通过Configuration保存String变量；<br>2、在reduce或者map阶段读取小文件存入内存，进行操作。</p></li><li><p>reduce阶段，<strong>==Iterable的迭代只能遍历一次==</strong>，第二次遍历后没有数据，所以会造成结果匹配错误。所以应该先将数据保存，再遍历。</p></li><li><p>set class 要在set configuration之后这样才会保存配置</p></li><li><p>如果reduce的Value不输出值得话，返回类型用NullWritable.get()，这样可以保证文件中在key的后面不会出现tab</p></li><li><p>Mapper Reducer类的子类要是static，否则会报初始化错误</p></li><li><p>==对于MR的return要谨慎使用，防止跳出，部分结果无法输出==</p></li></ul><h3 id="hdfs-操作指令"><a href="#hdfs-操作指令" class="headerlink" title="hdfs 操作指令"></a>hdfs 操作指令</h3><ul><li><p>hadoop fs -stat [option] hdfsfilepath</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%b：打印文件大小（目录为0）</span><br><span class="line">%n：打印文件名</span><br><span class="line">%o：打印block size （我们要的值）</span><br><span class="line">%r：打印备份数</span><br><span class="line">%y：打印UTC日期 yyyy-MM-dd HH:mm:ss</span><br><span class="line">%Y：打印自1970年1月1日以来的UTC微秒数</span><br><span class="line">%F：目录打印directory, 文件打印regular file</span><br></pre></td></tr></table></figure></li><li><p>hadoop job -status jobid job_1509011180094_5418072</p><p>结果说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">Uber job : false-----uber模式:false，Uber模式简单地可以理解成JVM重用。</span><br><span class="line">以Uber模式运行MR作业，所有的Map Tasks和Reduce Tasks将会在ApplicationMaster所在的容器（container）中运行，</span><br><span class="line">也就是说整个MR作业运行的过程只会启动AM container，因为不需要启动</span><br><span class="line">mapper 和reducercontainers，所以AM不需要和远程containers通信，整个过程简单了。</span><br><span class="line"></span><br><span class="line">Number of maps: 13702 -----map总数:</span><br><span class="line">Number of reduces: 500 -----reduces总数</span><br><span class="line">map() completion: 1.0  </span><br><span class="line">reduce() completion: 1.0  </span><br><span class="line">Job state: SUCCEEDED -----job状态  </span><br><span class="line">retired: false  </span><br><span class="line">reason for failure: -----failure原因  </span><br><span class="line"></span><br><span class="line">Counters: 58 -----counter总数  </span><br><span class="line">    File System Counters -----这个group表示job与文件系统交互的读写统计</span><br><span class="line">        FILE: Number of bytes read&#x3D;0 -----job读取本地文件系统的文件字节数。假定我们当前map的输入数</span><br><span class="line">        据都来自于HDFS，那么在map阶段，这个数据应该是0。但reduce在在执</span><br><span class="line">        行前，它的输入数据是经过Shuffle的merge后存储在reduce端本地磁盘</span><br><span class="line">        中，所以这个数据就是所有reduce的总输入字节数。</span><br><span class="line">        FILE: Number of bytes written&#x3D;5654852533 -----map的中间结果都会spill到本地磁盘中，在map执行完后，形成</span><br><span class="line">        最终的spill文件。所以map端这里的数据就表示MapTask往本地磁盘</span><br><span class="line">        中共写了多少字节。与Map端相对应的是，reduce端在Shuffle时，会</span><br><span class="line">        不断拉取Map端的中间结果，然后做merge并不断spill到自己的本地</span><br><span class="line">        磁盘中。最终形成一个单独文件，这个文件就是reduce的输入文件。</span><br><span class="line">        FILE: Number of read operations&#x3D;0 -----</span><br><span class="line">        FILE: Number of large read operations&#x3D;0</span><br><span class="line">        FILE: Number of write operations&#x3D;0</span><br><span class="line">        HDFS: Number of bytes read&#x3D;3560544443952 -----job执行过程中，累计写入HDFS的数据大小，整个job执行过程中</span><br><span class="line">        ，只有map端运行时，才会从HDFS读取数据，这些数据不限于源文件</span><br><span class="line">        内容，还包括所有map的split元数据。所以这个值应该比</span><br><span class="line">        FileInputFormatCounter.BYTES_READ要略大些。</span><br><span class="line">        HDFS: Number of bytes written&#x3D;317076139 -----Reduce的最终结果都会写入HDFS，就是一个Job执行结果的总量。</span><br><span class="line">        HDFS: Number of read operations&#x3D;70010</span><br><span class="line">        HDFS: Number of large read operations&#x3D;0</span><br><span class="line">        HDFS: Number of write operations&#x3D;4491</span><br><span class="line">        VIEWFS: Number of bytes read&#x3D;0</span><br><span class="line">        VIEWFS: Number of bytes written&#x3D;0</span><br><span class="line">        VIEWFS: Number of read operations&#x3D;0</span><br><span class="line">        VIEWFS: Number of large read operations&#x3D;0</span><br><span class="line">        VIEWFS: Number of write operations&#x3D;0</span><br><span class="line">        </span><br><span class="line">    Job Counters -----这个group描述与job调度相关的统计</span><br><span class="line">        Killed map tasks&#x3D;4</span><br><span class="line">        Launched map tasks&#x3D;13706 -----此job启动了多少个map task</span><br><span class="line">        Launched reduce tasks&#x3D;500 -----此job启动了多少个reduce task</span><br><span class="line">        Data-local map tasks&#x3D;13043 -----Job在被调度时，如果启动了一个data-local(源文件的副本在执行map task的TaskTracker本地)</span><br><span class="line">        Rack-local map tasks&#x3D;663 ----- 处理的文件不在map task节点上</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)&#x3D;437335720 -----所有map task占用slot的总时间，包含执行时间和创建&#x2F;销毁子JVM的时间</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)&#x3D;83963148 -----</span><br><span class="line">        Total time spent by all map tasks (ms)&#x3D;218667860</span><br><span class="line">        Total time spent by all reduce tasks (ms)&#x3D;27987716</span><br><span class="line">        Total vcore-seconds taken by all map tasks&#x3D;218667860</span><br><span class="line">        Total vcore-seconds taken by all reduce tasks&#x3D;27987716</span><br><span class="line">        Total megabyte-seconds taken by all map tasks&#x3D;557165707280</span><br><span class="line">        Total megabyte-seconds taken by all reduce tasks&#x3D;128631542736</span><br><span class="line">        </span><br><span class="line">    Map-Reduce Framework -----这个Countergroup包含了相当多的job执行细节数据。</span><br><span class="line">    这里需要有个概念认识是：一般情况下，record就表示一行数据，</span><br><span class="line">    而相对的byte表示这行数据的大小是多少，这里的group</span><br><span class="line">    表示经过reduce merge后像这样的输入形式&#123;&quot;aaa&quot;,[5,2,8,...]&#125;</span><br><span class="line">        &quot;Map input records&#x3D;4486906030&quot; -----所有MapTask从HDFS读取的文件总行数</span><br><span class="line">        Map output records&#x3D;93940285 -----MapTask的直接输出record是多少，就是在map方法中调用</span><br><span class="line">        context.write的次数，也就是未经过Combine时的原生输出条数。</span><br><span class="line">        Map output bytes&#x3D;5011599765 -----Map的输出结果key&#x2F;value都会被序列化到内存缓冲区中，</span><br><span class="line">        所以这里的bytes指序列化后的最终字节之和。</span><br><span class="line">        Map output materialized bytes&#x3D;3532812262 -----map 输出物化到磁盘的数据量，</span><br><span class="line">        也就是reduce shuffle的数据量</span><br><span class="line">        Input split bytes&#x3D;2839207</span><br><span class="line">        &quot;Combine input records&#x3D;93940285&quot; -----Combiner是为了尽量减少需要拉取和移动的数据</span><br><span class="line">        &quot;Combine output records&#x3D;79274144&quot; -----经过Combiner后，相同key的数据经过压缩，</span><br><span class="line">        在map端自己解决了很多重复数据，表示最终在map端中间</span><br><span class="line">        文件中的所有条目数</span><br><span class="line">        Reduce input groups&#x3D;13757989 -----Reduce总共读取了多少个这样的groups，</span><br><span class="line">        等于reduce处理的key个数</span><br><span class="line">        &quot;Reduce shuffle bytes&#x3D;3532812262&quot; -----Reduce端的copy线程总共从map端抓去了多少的中间数据</span><br><span class="line">        ，表示各个MapTask最终的中间文件总和。</span><br><span class="line">        &quot;Reduce input records&#x3D;79274144&quot; -----如果有Combiner的话，那么这里的数值就会等于Map端</span><br><span class="line">        Combiner运算后的最后条数，如果没有，那么就会等于Map的输出条数</span><br><span class="line">        Reduce output records&#x3D;0 -----所有reduce执行后输出的总条目数</span><br><span class="line">        &quot;Spilled Records&#x3D;79274144&quot; -----spill过程在map和reduce端都会发生，</span><br><span class="line">        这里统计的是总共从内存往磁盘中spill了多少条数据。</span><br><span class="line">        Shuffled Maps &#x3D;6851000 -----每个reduce几乎都得从所有Map端拉取数据，</span><br><span class="line">        每个copy线程拉取成功一个map的数据，那么增1，</span><br><span class="line">        所以它的总数基本等于reduce number*(map number - fiald)</span><br><span class="line">        Failed Shuffles&#x3D;0 -----copy线程在抓取map端中间数据时，</span><br><span class="line">        如果因为网络连接异常或是IO异常，所引起的Shuffle错误次数。</span><br><span class="line">        &quot;Merged Map outputs&#x3D;6851000&quot; -----记录着Shuffle过程中总共经历了多少次merge动作</span><br><span class="line">        &quot;GC time elapsed (ms)&#x3D;2890881&quot;</span><br><span class="line">        CPU time spent (ms)&#x3D;299372980 -----job运行使用的cpu时间，是衡量任务的计算量</span><br><span class="line">        总结：任务运行使用的CPU时间&#x3D;counter:</span><br><span class="line">        &quot;Map-Reduce Framework:CPU time spent (ms)&quot;</span><br><span class="line">        Physical memory (bytes) snapshot&#x3D;12848748335104 -----进程的当前物理内存使用大小</span><br><span class="line">        Virtual memory (bytes) snapshot&#x3D;45156357689344 -----进程的当前虚拟内存使用大小</span><br><span class="line">        Total committed heap usage (bytes)&#x3D;31420302491648 -----获取jvm的当前堆大小</span><br><span class="line">        </span><br><span class="line">     SHUFFLECOUNTER</span><br><span class="line">        SHUFFLE_IDLE_TIME&#x3D;21427585</span><br><span class="line">        SHUFFLE_TOTAL_TIME&#x3D;25507722</span><br><span class="line">        </span><br><span class="line">    HIVE</span><br><span class="line">        CREATED_FILES&#x3D;1</span><br><span class="line">            </span><br><span class="line">    Shuffle Errors -----这组内描述Shuffle过程中的各种错误情况发生次数，</span><br><span class="line">    基本定位于Shuffle阶段copy线程抓取map端中间数据时的各种错误。</span><br><span class="line">        BAD_ID&#x3D;0 -----每个map都有一个ID，</span><br><span class="line">        如attempt_201109020150_0254_m_000000_0，</span><br><span class="line">        如果reduce的copy线程抓取过来的元数据中的这个ID不是标准格式，</span><br><span class="line">        那么此Counter会增加。</span><br><span class="line">        CONNECTION&#x3D;0 -----表示copy线程建立到map端的连接有误。</span><br><span class="line">        IO_ERROR&#x3D;0 -----Reduce的copy线程如果在抓取map端数据时出现IOException，</span><br><span class="line">        那么这个值会相应增加。</span><br><span class="line">        WRONG_LENGTH&#x3D;0 -----map端的那个中间结果是有压缩好的有格式数据，</span><br><span class="line">        它有两个length信息：元数据大小和压缩后数据大小。</span><br><span class="line">        如果这两个length信息传输的有误，那么此Counter会增加。</span><br><span class="line">        WRONG_MAP&#x3D;0 -----每个copy线程当然是有目的的：为某个reduce抓取</span><br><span class="line">        某些map的中间结果，如果当前抓取的map数据不是copy</span><br><span class="line">        线程之前定义好的map，那么就表示把数据拉错了。</span><br><span class="line">        WRONG_REDUCE&#x3D;0 -----与上述描述一致，如果抓取的数据表示它不是</span><br><span class="line">        为此reduce而准备的，那还是拉错数据了。</span><br><span class="line">        DESERIALIZE_ERRORS&#x3D;0</span><br><span class="line">        </span><br><span class="line">    File Input Format Counters </span><br><span class="line">                Bytes Read&#x3D;0</span><br><span class="line">                </span><br><span class="line">    File Output Format Counters </span><br><span class="line">                Bytes Written&#x3D;0</span><br></pre></td></tr></table></figure><h3 id="hadoop优化"><a href="#hadoop优化" class="headerlink" title="hadoop优化"></a>hadoop优化</h3><h4 id="mr-job-流程"><a href="#mr-job-流程" class="headerlink" title="mr job 流程"></a>mr job 流程</h4><p><img src="https://note.youdao.com/yws/api/personal/file/3A6283D2710445489A224B6E54B1B58B?method=download&shareKey=fb4df9fb0f57939d165fd36486723d88" alt=""></p><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4></li></ul><h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><ul><li><p>io.sort.mb:100m</p><ol><li>存储map中间数据的缓存默认大小,当map任务产生了非常大的中间数据时可以适当调大该参数,使缓存能容纳更多的map中间数据，而不至于大频率的IO磁盘,当系统性能的瓶颈在磁盘IO的速度上,可以适当的调大此参数来减少频繁的IO带来的性能障碍。</li><li>查看日志，spill次数多说明设置太低。（根据map的输出量进行设置）</li></ol></li><li><p>io.sort.spill.percent:80%</p><p>达到一定百分比，从后台进程对buffer进行排序，然后spill到磁盘。如果map的输出基本有序可以适当提高这个阈值。</p></li><li><p>io.sort.factor:10</p><ol><li>最多能有多少并行的stream向merge文件中写入</li><li>当一个map task执行完之后，本地磁盘上(mapred.local.dir)有若干个spill文件，map task最后做的一件事就是执行merge sort，把这些spill文件合成一个文件（partition），有时候我们会自定义partition函数，就是在这个时候被调用的。</li><li>merge sort会生成两个文件，一个是数据文件，一个是index：记录每个不同的key在数据文件中的偏移量（这就是partition）</li></ol></li><li><p>min.num.spill.for.combine:3</p><p>当job中设定了combiner，并且spill数最少有3个的时候，<br>那么combiner函数就会在merge产生结果文件之前运行。减少写入到磁盘文件的数据数量，同样是为了减少对磁盘的读写频率，有可能达到优化作业的目的。</p></li><li><p>mapred.compress.map.output:false</p><p>那么map在写中间结果时，就会将数据压缩后再写入磁盘，读结果时也会采用先解压后读取数据。cpu换IO</p></li><li><p>mapred.map.output.compression.codec:org.apache.hadoop.io.compress.De faultCodec(GzipCodec，LzoCodec，BZip2Codec，LzmaCodec)<br>当采用map中间结果压缩的情况下，用户还可以选择压缩时采用哪种压缩格式进行压缩</p></li></ul><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><h4 id="优化场景"><a href="#优化场景" class="headerlink" title="优化场景"></a>优化场景</h4><ul><li><p>Map逻辑处理后数据被展开，写磁盘次数剧增，可以观察日志中的spill次数，调整各个参数</p></li><li><p>中间结果能不展开就不展开，尽量缩小Mapper和reducer之间的数据传递</p></li><li><p>处理速度很慢时候首先要怀疑Mapper和Reducer之间传输数据量过大</p></li><li><p>观察GC的情况，有时候是因为内存占用量高，频繁GC，严重影响处理速</p></li><li><p>适当控制mapper的数量，特别是有distribute cache的场景</p></li><li><p>distribute cache</p><ul><li>加载的数据能不用hashmap就尽量不要用，hashmap会使得内存占用量是原数据的5-10倍。</li><li>加载的数据要尽可能简单，如果有复杂的处理逻辑可以单独开辟Mapper Reducer进行一轮处理</li><li>避免每次mapper都要处理一遍，尽可能减少distribute cache的数据量</li></ul></li></ul><h3 id="hadoop配置说明"><a href="#hadoop配置说明" class="headerlink" title="hadoop配置说明"></a>hadoop配置说明</h3><blockquote><p>tasktracker.http.threads：</p></blockquote><pre><code>决定作为server端的map用于提供数据传输服务的线程数  </code></pre><blockquote><p>mapred.reduce.parallel.copies：</p></blockquote><pre><code>决定作为client端的reduce同时从map端拉取数据的并行度（一次同时从多少个map拉数据）</code></pre><hr><h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><h3 id="notice-1"><a href="#notice-1" class="headerlink" title="notice"></a>notice</h3><ul><li><p>hive 查询，字段无法进行单引号比较，如p8=’2’，无法查到数据</p><p>当某一字段数据全是纯数字字符串的时候，它会自动转成数字去做比较。</p></li><li><p>10位的时间戳值，即1970-1-1至今的秒，可以用<code>from_unixtime()</code>转为时间，而13位的所谓毫秒的是不可以的;<code>from_unixtime(cast(substring(t3.time ,1,10)as BIGINT), &#39;yyyyMMdd HH:mm:ss&#39;)</code></p></li><li><p>insert overwrite/into table 只会有mapjob，没有reducejob</p></li><li><p>增加reduce数目</p><ol><li>set hive.exec.reducers.bytes.per.reducer=500000000;</li><li>set mapred.reduce.tasks = 15;</li></ol></li><li><p>设置reduce的文件大小<br>set hive.merge.size.per.task = 10000000;<br>set hive.merge.mapfiles=false;<br>set hive.groupby.skewindata=true;</p></li></ul><h3 id="hql"><a href="#hql" class="headerlink" title="hql"></a>hql</h3><ul><li>desc tablename;</li><li>show partitions tablename;</li><li>修改表名</li></ul><blockquote><p>alter table oldname rename to newname;</p></blockquote><ul><li>增加列</li></ul><blockquote><p>alter table tablename add columns (c1 type, c3 type);</p></blockquote><ul><li><p>hive增加分区映射到文件</p><blockquote><p>alter table tablename drop if exists partition(dt=’20171130’);<br>alter table tablename add if not exists partition(dt=’20171130’) location ‘path’;</p></blockquote></li><li><p>修改表的分割字符</p></li></ul><blockquote><p>alter table tablename set SERDEPROPERTIES(‘field.delim’=’\t’);</p></blockquote><ul><li><p>修改字段顺序</p><blockquote><p>alter table tablename change column cololdname colnewname coltype after colname2;</p></blockquote></li></ul><h3 id="hive-和hadoop-配置说明"><a href="#hive-和hadoop-配置说明" class="headerlink" title="hive 和hadoop 配置说明"></a>hive 和hadoop 配置说明</h3><h4 id="hive-配置说明"><a href="#hive-配置说明" class="headerlink" title="hive 配置说明"></a>hive 配置说明</h4><h4 id="hadoop-配置说明"><a href="#hadoop-配置说明" class="headerlink" title="hadoop 配置说明"></a>hadoop 配置说明</h4><blockquote><p>mapred.compress.map.output ##指定map的输出是否压缩。有助于减小数据量，减小io压力，但压缩和解压有cpu成本，需要慎重选择压缩算法。</p></blockquote><blockquote><p>mapred.map.output.compression.codec ##map输出的压缩算法</p></blockquote><blockquote><p>mapred.output.compress ##reduce输出是否压缩</p></blockquote><blockquote><p>mapred.output.compression.codec ##控制mapred的输出的压缩的方式</p></blockquote><blockquote><p>hive.exec.compress.intermediate=true; ##hive中间数据压缩</p></blockquote><blockquote><p>set hive.exec.compress.intermediate=true;<br>set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec<br>set mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</p></blockquote><blockquote><p>set hive.exec.compress.output=false;</p></blockquote><h2 id="hive-reduce最终数据不压缩"><a href="#hive-reduce最终数据不压缩" class="headerlink" title="hive reduce最终数据不压缩"></a>hive reduce最终数据不压缩</h2><blockquote><p>set hive.exec.compress.output=true;<br>set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</p></blockquote><blockquote><p>set hive.merge.smallfiles.avgsize=256000000; ###设置输出文件的平均值</p></blockquote><h3 id="hive-Job-细节优化"><a href="#hive-Job-细节优化" class="headerlink" title="hive Job 细节优化"></a>hive Job 细节优化</h3><ul><li>map</li></ul><blockquote><p>set mapred.min.split.size=1;<br>set mapred.max.split.size=256000000;</p></blockquote><ul><li>reduce</li></ul><blockquote><p>set mapred.reduce.tasks=100;–直接指定Reduce个数<br>set mapred.exec.reducers.bytes.per.reducer=1G;</p></blockquote><ul><li>map 与reduce过程中</li></ul><blockquote><p>set io.sort.mb;–增大Mapper输出buffer的大小，避免Spill的发生<br>set io.sort.factor;–一次性能够合并更多的数据<br>set sort mapred.reduce.slowstart.completed.maps=0.05;–Reduce端copy的进度<br>set mapred.reduce.parallel.copies;–可以决定作为client端的Reduce同时从Map端拉取数据的并行度</p></blockquote><ul><li>文件格式</li></ul><blockquote><p>set hive.default.fileformat = SequenceFile;<br>set hive.exec.compress.output = true;</p></blockquote><p>对于sequencefile，有record和block两种压缩方式可选，block压缩比更高</p><blockquote><p>set mapred.output.compression.type = BLOCK;<br>set hive.hadoop.supports.splittable.combineinputformat=true;–小文件合并</p></blockquote><ul><li>Job 整体优化<ul><li>job 执行模式</li></ul></li></ul><blockquote><p>set hive.exec.mode.local.auto;–自动开启local mr模式<br>set hive.exec.mode.local.auto.tasks.max;–文件数量<br>set hive.exec.mode.local.auto.inputbytes.max;–数据量大小</p></blockquote><ul><li>jvm重用</li></ul><blockquote><p>set mapred.job.reuse.jvm.num.tasks=5;–一个jvm运行多次任务之后再退出</p></blockquote><ul><li>索引</li><li>join</li></ul><blockquote><p>set hive.auto.convert.join = true;</p></blockquote><p>Hive会自动判断当前的join操作是否合适做Map join</p><ul><li>数据倾斜</li></ul><blockquote><p>set hive.map.aggr=true;<br>set hive.groupby.skewindata;</p></blockquote><p>Reduce操作的时候，拿到的key并不是所有相同值给同一个Reduce，而是随机分发，然后Reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果</p><ul><li><p>sql 整体优化</p><ul><li><p>job 并行<br>set hive.exec.parallel = true;<br>set hive.exec.parallel.thread.number;</p></li><li><p>减少Job数<br>group by 代替 join</p></li></ul></li></ul><hr></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:wuzhijunw@163.com">wufish</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://wufish.github.io/2018/11/29/bigdata/hadoop&amp;hive%E5%8F%82%E6%95%B0/">https://wufish.github.io/2018/11/29/bigdata/hadoop&amp;hive%E5%8F%82%E6%95%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://wufish.github.io" target="_blank">wufish的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/learning/">learning</a><a class="post-meta__tags" href="/tags/hadoop/">hadoop</a><a class="post-meta__tags" href="/tags/hive/">hive</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechat.png" target="_blank"><img class="post-qr-code-img" src="/images/wechat.png" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/images/alipay.png" target="_blank"><img class="post-qr-code-img" src="/images/alipay.png" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2018/11/29/summary/mysummary/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">实践总结</div></div></a></div><div class="next-post pull-right"><a href="/2018/11/29/bigdata/learningscala/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">学习记录-scala</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i> <span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2018/11/29/bigdata/learningMR/" title="学习记录-MR"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-11-29</div><div class="title">学习记录-MR</div></div></a></div><div><a href="/2019/12/21/docker/learningdocker/" title="学习记录-Docker"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-12-21</div><div class="title">学习记录-Docker</div></div></a></div><div><a href="/2018/11/29/git/learninggit/" title="学习记录-Git"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-11-29</div><div class="title">学习记录-Git</div></div></a></div><div><a href="/2018/11/29/idea/IntelliJ IDEA keymap/" title="IntelliJ IDEA快捷键"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-11-29</div><div class="title">IntelliJ IDEA快捷键</div></div></a></div><div><a href="/2020/10/04/interview/服务稳定性和高可用/" title="服务稳定性和高可用"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-04</div><div class="title">服务稳定性和高可用</div></div></a></div><div><a href="/2018/11/29/linux/learningshell/" title="学习记录-shell"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-11-29</div><div class="title">学习记录-shell</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 By wufish</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49b1f5">hexo-generator-search</a> <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk({
      clientID: '233588b013801805cef1',
      clientSecret: 'cfe68da267d8ad8e1b430f8a616e5a0536562aad',
      repo: 'wufish.github.io',
      owner: 'wufish',
      admin: ['wufish'],
      id: '8a8bf0b6655bdbf017f9ab9d1005ce5a',
      language: 'zh-CN',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    })
    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    $.getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js', initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>